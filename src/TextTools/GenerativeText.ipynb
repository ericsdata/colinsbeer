{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericsdata/colinsbeer/blob/main/src/TextTools/GenerativeText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL0-UTdgRPDm"
      },
      "source": [
        "## Generating New Reivews\n",
        "\n",
        "Using a selection of beer revies from the Beer Ratings dataset, we are going to try out generate new text reviews on beer and / or brewer cues.  We will try out several language models - including GPT 2 for text generation. \n",
        "\n",
        "The file `write_txt_train.py` has detailed information in how the training set was compiled. Overall, beers that scored at a 4.0 or higher were considered \"good\".\n",
        "\n",
        "Resource : https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=U_XJVIetKN-h\n",
        "\n",
        "Resource 2: https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsE6MGXkR6Xk",
        "outputId": "619423e3-c98c-4357-eedf-ce15b496cb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "## Environment will require HF transformers package\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLj35eCMSwMk"
      },
      "source": [
        "### Data read in\n",
        "\n",
        "Working in this colab environment requires a manual upload of the dataset. As google will remind you, this upload expires at the end of the session, so must be reuploaded each time the user runs this notebook. \n",
        "\n",
        "Dataset contains beers with at least 30 reviews "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yCbB5_CjAY1e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "## Csv produced by write_txt_train.py file\n",
        "#dat = pd.read_csv(r'..\\txt_train.csv')\n",
        "#dat.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2bb8vZ6Tg3W"
      },
      "source": [
        "### Model Loading \n",
        "\n",
        "We are relying on HF models to deploy this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mE5wd3QhKa59",
        "outputId": "7cf9b058-21b8-4347-824f-8b1aec08c508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "## GPT2 Doesnt have pad token??\n",
        "#### Errored out later unless this was added\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l9Mj17KpKa5_"
      },
      "outputs": [],
      "source": [
        "### FAKE DATA\n",
        "\n",
        "revs = [\"On tap at the Springfield, PA location. Poured a deep and cloudy orange (almost a copper) color with a small sized off white head. Aromas or oranges and all around citric. Tastes of oranges, light caramel and a very light grapefruit finish. I too would not believe the 80+ IBUs - I found this one to have a very light bitterness with a medium sweetness to it. Light lacing left on the glass.\",\n",
        "\"On tap at the John Harvards in Springfield PA.  Pours a ruby red amber with a medium off whie creamy head that left light lacing.  Aroma of orange and various other citrus.  A little light for what I was expecting from this beers aroma...expecting more from the Simcoe.  Flavor of pine, orange, grapefruit and some malt balance.  Very light bitterness for the 80+ IBUs they said this one had.\",\n",
        "\"UPDATED: FEB 19, 2003 Springfield, PA. I've never had the Budvar Cristal but this is exactly what I imagined it to be.  A clean and refreshing, hoppy beer, med bodied with plenty of flavor.  This beer's only downfall is an unpleasant bitterness in the aftertaste.\",\n",
        "\"On tap the Springfield PA location billed as the Fancy Lawnmower Light.  Pours a translucent clear yellow with a small bubbly white head.  Aroma was lightly sweet and malty, really no hop presence.  Flavor was light, grainy, grassy and malty.  Just really light in flavor and aroma overall. Watery.\",\n",
        "\"On tap at the Springfield, PA location. Poured a lighter golden color with a very small, if any head. Aromas and tastes of grain, very lightly fruity with a light grassy finish. Lively yet thin and watery body. Oh yeah, the person seating me told me this was a new one and was a Pale Ale even though the menu he gave me listed it as a lighter beer brewed in the Kolsh style.\",\n",
        "\"Springfield, PA location... Poured an opaque black color with a creamy tan head and nice lacing.  Strong vanilla and roasted malt aroma.  Creamy taste of coffee, chocolate and vanilla. The bartender told me this was an imperial stout at about 8%.  She didn't convince me, there was no alcohol to be found, and it was sweet as hell!  But still good.\",\n",
        "\"On tap at the Springfield, PA location. Listed on the beer menu as \"\"James Brown Ale\"\". Had the regular and cask version. Poured a deep brown color with an averaged sized off white head (cask had a huge head). Ill stop on the cask version here as I found it to smell and taste like buttered popcorn. The regular had aromas of nuts, light chocolate, and roast. Taste of chocolate, nuts, very light roast and caramel.\t Tasted on 9/7/2006 and moved over as part of the John Harvard clean up.\"\",\"\n",
        "\"Sampled @ the Springfield, PA location.   Candi Sugar dominates this Belgian Ale.  Beer was on the flat side but had a nice crimson color.   Enjoyable Belgian Ale, I did not expect John Harvards to have it in its line-up.\",\n",
        "\"Springfield... Poured a hazy copper color with a medium sized, off white head that left spotty lacing on the glass.  Aroma of yeast, dried fruits, clove, banana, and cherries, with light roastiness.  Aroma was very dubbelish.  Herbal taste of dark fruits, yeast and alcohol was barely noticed.  Slick mouthfeel.  Could have been more flavorful.\",\n",
        "\"UPDATED: FEB 19, 2003 Springfield, PA. Darkish copper colored, with no head -\tprobably poured like that on purpose.\tServed inappropriately at about 40 deg\tF.  This beer was cold.  It tasted\tfine at that temp but I had to let it warm up \tfor awhile.  It was worth the wait, as a\tvery interesting and complex character developed.\tVery phenolic and funky - with a strong ester of\tbubblegum.  Also a little clove or some kind of \tspice.  Strong but not overwhelming at all.  Surpisingly\teasy to drink.\"\",\"\n",
        "\"UPDATED: FEB 19, 2003 Springfield, PA. Sharp and cloyingly sweet.  The alcohol presence becomes more and more noticeable.\",\n",
        "\"UPDATED: FEB 19, 2003 Springfield, PA. Interesting example.  The fruit flavors are very apparent, but the natural mildness of the currants keep the sweetness in check.  These flavors blend well with the white beer base.\",\n",
        "\"From Springfield PA:  nice smooth\tmalty flavor, mildy fruity, but served\tvia nitro and thru a restrictor disc\t(stout tap).  Thus, overly creamy and lacking\tsome of its original flavor.  I could tell there\twas a pretty good beer in there.  Aroma difficult\tto detect.\"\",\"\n",
        "\"On tap at Springfield location.  Pours a translucent golden amber with really no head.  Aroma of caramel, grains and light hops.  Flavor was malty with a light hop presence.  Really kind of non-descript overall.\",\n",
        "\"On tap at the Springfield, PA location. Poured a medium and see through orange color with a small sized off white head. Aromas and tastes on the weak side and contained some citrus, caramel, and grains. Body was thin and watery.\",\n",
        "\"Handbottled from trade wth Sprinkle. Pours a nice dark copper color with medium size off white head. Aroma of bourbon, malt , hops and oak. Slight smokey flavor with a bourbon taste in the initial sip. Flavors of malt, vanilla and hops still remain although none dominate the brew. Taste is still very enjoyable with a smooth and balanced finish.\",\n",
        "\"On tap at the Great Taste of the Midwest (8/9/08): Pours a transparent bright copper orange with an airy white head.  Aroma of sweet toasty pale malt and sweet light fruitiness with a good resiny piny hop character.  Body starts with decent fullness and sweet caramel malts with good balance of hop flavor and bitterness.  Finishes smooth and bittersweet, nicely aged and balanced.\",\n",
        "\"UPDATED: JUL 7, 2009 On tap. Interesting experiment, but I liked regular Hopula better. Pours a dark amber with an off white head. Aroma is slightly wood and hops, but mostly bourbon. Flavor is everything I loved about Hopula, but with too much booze. Some nice vanilla notes, but bourbon constantly overpowers the Hopula. Just not my style, but would be very interesting if they reused the barrels.\",\n",
        "\"On cask at BI - Aroma of the Hopula Play-Doh hops and malt with lots of oak, vanilla and bourbon.  Pours dark mahogany with a medium lasting head and great lacing.  Flavor is strong bourbon, too strong.  The base beer is hidden under there somewhere but is way overpowered.  I had trouble getting it down to be honest.\",\n",
        "\"GTMW 08 on cask - Pours bronze orang with a minimal head.  The aroma has lots of vanilla and some bourbon and toasty malt.  Medium sticky body with light carbonation.  The flavor starts with the aroma traits with more caramel malt and earthy hops.  The finish has vanilla and okay dominating everything.  This kind of beer is just not down my alley.\"\n",
        "\n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tQAGmnS9mDrL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(revs))\n",
        "val_size = len(revs) - train_size\n",
        "\n",
        "revs_train, revs_val = random_split(revs, [train_size, val_size])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TXz15mOm4DU"
      },
      "source": [
        "I define the generative Beer Data. The class takes a text list, adds beginning / ending tags to each element, then tokenizes. \n",
        "It returns input_id and attention mask layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iXPcm9Jqa6Cm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class generative_BD(torch.utils.data.Dataset):\n",
        "  '''Sequence text tokens\n",
        "      This means it adds tags to start and end of texts\n",
        "\n",
        "      Reads in text dataset, & tokenizes\n",
        "\n",
        "      !!! NEED TO ADD PADDING TOKEN\n",
        "  '''\n",
        "\n",
        "  def __init__(self,text_list, tokenizer, text_tags, gpt2_type=\"gpt2\", max_length=768):\n",
        "    #self.text_list = text_list\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "    self.text_tags = [text_tags]\n",
        "\n",
        "    ## In definitoin\n",
        "    for txt in text_list:\n",
        "\n",
        "        encodings_dict = tokenizer('<%s>'%(text_tags[0])+ txt + '<%s>'%(text_tags[1]), truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "        self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "        self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "\n",
        "\n",
        "class Beer_Class_Data(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Classification torch data set, each record has encodings and labels\n",
        "  '''\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoU76-pkZOmF"
      },
      "source": [
        "We will used a pretrained distilbert model for this task, fine tuning it for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HM69rIbtms14"
      },
      "outputs": [],
      "source": [
        "train = revs_train\n",
        "train_dataset = generative_BD(text_list= train, tokenizer = tokenizer, text_tags = ['CLS', 'SEP'])\n",
        "\n",
        "val_dataset = generative_BD(text_list= revs_val, tokenizer = tokenizer, text_tags = ['CLS', 'SEP'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "# Create the DataLoaders for our training and validation datasets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "NpKhn3IlLEyZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0AIcB8u4nn17",
        "outputId": "7618527e-62e4-44bd-9b6c-2e45896ad35f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   27,  5097,    50,    29,    52, 49316,    25, 18630,    33,   678,\n",
              "            11,  5816, 27874,    11,  8147,    13,  3801,   680, 15317, 16396,\n",
              "            11,   351,   645,  1182,   532,   197, 26949, 19036,   588,   326,\n",
              "           319,  4007,    13,   197,    50,  8520, 49190,   379,   546,  2319,\n",
              "          3396,   197,    37,    13,   220,   770,  6099,   373,  4692,    13,\n",
              "           220,   632, 29187,   197, 38125,   379,   326, 20218,   475,   314,\n",
              "           550,   284,  1309,   340,  5814,   510,   220,   197,  1640, 27416,\n",
              "            13,   220,   632,   373,  2861,   262,  4043,    11,   355,   257,\n",
              "           197,   548,  3499,   290,  3716,  2095,  4166,    13,   197, 16371,\n",
              "          6566,  4160,   290, 42958,   532,   351,   257,  1913,  1658,   353,\n",
              "           286,   197, 46176,   903,    70,   388,    13,   220,  4418,   257,\n",
              "          1310,   537,   659,   393,   617,  1611,   286,   220,   197,  2777,\n",
              "           501,    13,   220, 13535,   475,   407,  9721,   379,   477,    13,\n",
              "           220,  4198,    79,  1710,   306,   197, 38171,   284,  4144,  1539,\n",
              "            52, 49316,    25, 18630,    33,   678,    11,  5816, 27874,    11,\n",
              "          8147,    13, 22835,   290,   537,   726,  4420,  6029,    13,   220,\n",
              "           383,  5548,  4931,  4329,   517,   290,   517, 19787, 29847,  5188,\n",
              "            47,    29, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]),\n",
              " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_dataset.__getitem__(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxvfn8dnKa6O"
      },
      "source": [
        "GPT2 model is very large - use on a CPU will likely cause memory errors. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5k0CJE6pKa6P"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# I'm not really doing anything with the config \n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9-qTVZRKa6Q"
      },
      "source": [
        "Function written below is used to \"accumulate the gradients\". The idea being that before optimizing the step of gradient descent - the algorithm will first evaluate the gradient of several potential operations. It divides that sum by the number of accumulated steps, and finally gets an average loss over the training sample. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-57hhf3oKa6Q"
      },
      "outputs": [],
      "source": [
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HPj06BvXKa6S"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "n8L-6OnWKa6T"
      },
      "outputs": [],
      "source": [
        "## Parameters\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gcOyAddgKa6X",
        "outputId": "d49ba3c3-500d-4ccf-9eef-c2002099465c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "            lr = learning_rate,\n",
        "            eps = epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ru58_ORVKa6Y"
      },
      "outputs": [],
      "source": [
        "### Figure out how many training steps\n",
        "\n",
        "total_steps = len(train) * epochs\n",
        "\n",
        "## A scheduler adjusts the learning rate as the training loop progresses\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GL3Ze64AKa6Z"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import time\n",
        "\n",
        "def format_time(time_elapsed):\n",
        "    return str(datetime.timedelta(seconds = int(round(time_elapsed))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YaHGjtUKKa6Z",
        "outputId": "f07dc45e-43f4-4240-87a5-445f484d682b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of     15. Loss: 0.23826713860034943.   Elapsed: 0:00:03.\n",
            "0:  reproductiveCLS>On tap at the Springfield, PA location.       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of     15. Loss: 0.34713104367256165.   Elapsed: 0:00:06.\n",
            "0:  zone.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of     15. Loss: 0.25812578201293945.   Elapsed: 0:00:03.\n",
            "0:  commitsCLS>On tap at the Springfield, PA location. Pours a translucent amber color with\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of     15. Loss: 0.3524359166622162.   Elapsed: 0:00:06.\n",
            "0:  irony.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of     15. Loss: 0.2501694858074188.   Elapsed: 0:00:03.\n",
            "0:  SahCLS>On tap at the Springfield, PA location. Pours a translucent amber with a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of     15. Loss: 0.3535568416118622.   Elapsed: 0:00:06.\n",
            "0:  BryanCLS>On tap at the Springfield, PA location. Pours a translucent amber color with\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of     15. Loss: 0.26858657598495483.   Elapsed: 0:00:03.\n",
            "0:  spirits.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of     15. Loss: 0.3470805883407593.   Elapsed: 0:00:06.\n",
            "0:  sees the\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch     5  of     15. Loss: 0.24757374823093414.   Elapsed: 0:00:03.\n",
            "0:  hungry.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    10  of     15. Loss: 0.35396137833595276.   Elapsed: 0:00:06.\n",
            "0:  PTCLS>On tap at the Springfield, PA location. Pours a translucent amber color with\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0:00:09\n",
            "\n",
            "Running Validation...\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:46 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "#start training\n",
        "total_t0 = time.time()\n",
        "## store records of each run\n",
        "training_stats = []\n",
        "\n",
        "### Manual set for each epoch of training\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    ## Store process starting time\n",
        "    startT = time.time()\n",
        "    ## Init loss across training\n",
        "    total_train_loss = 0\n",
        "    #begin model training\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataset):\n",
        "        b_input_ids =batch[0].to(device) ## send input ids to model\n",
        "        b_labels = batch[0].to(device) ## send labels to model (labs are smae as input in gen text)\n",
        "        b_masks = batch[1].to(device) ## send attention layer to model\n",
        "\n",
        "        #init gradient at 0\n",
        "        model.zero_grad()        \n",
        "        ## feed labels, inputs, and masks to model\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "        ## Calc loss\n",
        "        loss = outputs[0]  \n",
        "        ### loss on batch\n",
        "        batch_loss = loss.item()\n",
        "        ## Add it to total training loss\n",
        "        total_train_loss += batch_loss\n",
        "        ### Reporting step\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "          elapsed = format_time(time.time() - startT)\n",
        "          print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataset), batch_loss, elapsed))\n",
        "\n",
        "          model.eval()\n",
        "          ### Output some samples so you know its working\n",
        "          sample_outputs = model.generate(\n",
        "                                bos_token_id = random.randint(1,30000)\n",
        "                                ,do_sample = True\n",
        "                                ,top_k = 50\n",
        "                                ,max_lenght = 200\n",
        "                                ,top_p = 0.095\n",
        "                                ,num_return_sequences = 1\n",
        "          )\n",
        "\n",
        "          for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "          model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "    ## Calcu average loss over all epochs\n",
        "    avg_train_loss = total_train_loss / len(train_dataset)\n",
        "\n",
        "    ## Ouput how long epoch took\n",
        "    training_time = format_time(time.time() - startT)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "     # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs  = model(b_input_ids, \n",
        "#                            token_type_ids=None, \n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "          \n",
        "            loss = outputs[0]  \n",
        "            \n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss        \n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)    \n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "aample_outputs\n",
        "#for i, sample_output in enumerate(aample_outputs):\n",
        "#  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "yAjPhHkgMORU",
        "outputId": "8c8f1488-6d88-4316-c0f1-91c45df1221c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9116,  5097,    50,    29,  2202,  9814,   379, 27874,    11,  8147,\n",
              "          4067,    13,   220,   220,   350,  4662,   257,  2042,  1090,    83,\n",
              "          4712,   326,  4940,   572,   262, 12658,   414,    11,   475,  4622,\n",
              "           572, 12658,   414,    11, 26868, 26868,  1182,    13,   220,   317,\n",
              "         42902,   318,   286, 16858,   290, 16858,    11,   286,  7090, 16429,\n",
              "           798,   290,  1657, 32595,   351,   257, 11607,   284,  7090, 32595,\n",
              "         36410,    13,   220, 13398, 16429,   798,  1657, 32595,  1107,  1657,\n",
              "           290, 29438,  7833,   220, 13398, 32595,   351,   617,  8701,    88,\n",
              "         29438,  4931,    11,   257,  1657,  1725, 35987,   290,  1725,  4931,\n",
              "            13,   220,   317,   398,   292,   290, 16858,   550, 32595, 21824,\n",
              "           351,   617, 12658,   414,  1108,   290, 35987,   290,   617,  8701,\n",
              "            88,  1725,  4931,    13,   220,  2773,   286,   262,  9565,   286,\n",
              "         15921,   290, 14380,   468,   257, 48666,  7277, 31242,   290,   257,\n",
              "         11607, 32595,   680,  1658,   353, 32595,    11,   300,  4092,   290,\n",
              "         42958, 13020,  1272,    13,   220, 26438,   373,  3716,   290,  8234,\n",
              "            88,    13,   220,  2329,   287,    11, 41855, 38777,   262, 41855,\n",
              "            13,   220, 29633,   286, 41855,   468,   587,  4166,    11,   351,\n",
              "           220, 16858,   290, 16858,  1658,   353,   290, 32913,  4710,    13,\n",
              "           220,   383,  9565,   286,  6891,   468,   550,  1913, 12658,   414,\n",
              "          1725, 35987,    13,   220,   383, 42902,   286,   262,  6099,   318]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"<|startoftext|>\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                #bos_token_id=random.randint(1,30000),\n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 300,\n",
        "                                top_p=0.95, \n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "uanEY0VUNZYQ",
        "outputId": "27893781-6b1c-42c6-c36c-eab92978013a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  27,   91, 9688, 1659, 5239,   91,   29]], device='cuda:0')\n",
            "0: <|startoftext|>Served, PA.  any beer at the Hopula tastes exactly as if it had been aged, tasted, and served on tap.  Interesting.  Aroma has a thin, cloyingly fruity, cloyingly fruiny beer with some of what would have been beer had it been more light.<SEP>\n",
            "\n",
            "\n",
            "1: <|startoftext|> Flavor starts at the very base of the Bud Light beer. Pours creamy orange with caramel and roast balance.<SEP>\n",
            "\n",
            "\n",
            "2: <|startoftext|>CLS., clasps the I've lacing of my left hand. Hop down the Pours a medium sized bronze with one head that starts with an orange and tan head. These cask alcohol flavors are overwhelming and overwhelming.<SEP>\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BeerSentiment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}